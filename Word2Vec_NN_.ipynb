{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BRUmA4uiR3-M"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, hamming_loss, coverage_error, confusion_matrix\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.preprocessing import MultiLabelBinarizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/DeepikaPanneer/Multi-Label-Predictions-on-Academic-Articles/main/data/train.csv')\n",
        "data['TEXT'] = data['TITLE'] + ' ' + data['ABSTRACT']\n",
        "\n",
        "X = data['TEXT']\n",
        "y = data[['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']].values\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenization\n",
        "max_words = 10000\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
        "\n",
        "maxlen = 200\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=maxlen)\n",
        "X_val_padded = pad_sequences(X_val_seq, maxlen=maxlen)\n",
        "\n",
        "# Compute class weights for multi-label data\n",
        "def compute_multi_label_class_weights(y):\n",
        "    n_samples = len(y)\n",
        "    n_classes = y.shape[1]\n",
        "\n",
        "    class_counts = np.sum(y, axis=0)\n",
        "    class_weights = n_samples / (n_classes * class_counts)\n",
        "\n",
        "    return class_weights\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_multi_label_class_weights(y_train)\n"
      ],
      "metadata": {
        "id": "2gTC33qgXiS0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained Word2Vec embeddings\n",
        "word2vec_path = 'https://drive.google.com/file/d/1CBfeuD2OeFynUwLnIS1ft7ZiqLC48B5K/view?usp=drive_link'\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u9hQGPwzXnU6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import datapath\n",
        "%cd /content/drive/My Drive/Ranjitha/\n",
        "word2vec_model = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gqxfXiPqZMST",
        "outputId": "40f319d4-6e0b-45eb-9a5b-081327d2efa3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "/content/drive/My Drive/Ranjitha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.initializers import Constant\n",
        "# Prepare embedding matrix\n",
        "embedding_dim = 300\n",
        "num_words = min(max_words, len(tokenizer.word_index)) + 1\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i > max_words:\n",
        "        continue\n",
        "    if word in word2vec_model:\n",
        "        embedding_matrix[i] = word2vec_model[word]\n",
        "\n",
        "\n",
        "\n",
        "# Define the neural network model\n",
        "model = Sequential()\n",
        "model.add(Embedding(num_words, embedding_dim, embeddings_initializer=Constant(embedding_matrix),\n",
        "                    trainable=False))  # Freeze the embedding layer\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(6, activation='sigmoid'))  # Sigmoid activation for multi-label classification\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',  # Binary crossentropy for multi-label classification\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZUKnpHZBXq2x",
        "outputId": "27abb1fc-a931-41e1-a560-769449e77ed4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, None, 300)         3000300   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               219648    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 6)                 390       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3228594 (12.32 MB)\n",
            "Trainable params: 228294 (891.77 KB)\n",
            "Non-trainable params: 3000300 (11.45 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of class weights\n",
        "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_padded, y_train, epochs=10, batch_size=32, validation_data=(X_val_padded, y_val),class_weight=class_weight_dict)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UfJg8bPmXutp",
        "outputId": "30f40fbd-07cc-47e0-9324-9af54aff1974"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "525/525 [==============================] - 405s 751ms/step - loss: 0.3479 - accuracy: 0.3951 - val_loss: 0.3905 - val_accuracy: 0.6110\n",
            "Epoch 2/10\n",
            "525/525 [==============================] - 394s 750ms/step - loss: 0.3038 - accuracy: 0.5028 - val_loss: 0.4056 - val_accuracy: 0.4319\n",
            "Epoch 3/10\n",
            "525/525 [==============================] - 397s 757ms/step - loss: 0.2629 - accuracy: 0.5682 - val_loss: 0.3966 - val_accuracy: 0.5263\n",
            "Epoch 4/10\n",
            "525/525 [==============================] - 399s 760ms/step - loss: 0.2226 - accuracy: 0.6471 - val_loss: 0.3188 - val_accuracy: 0.6396\n",
            "Epoch 5/10\n",
            "525/525 [==============================] - 396s 754ms/step - loss: 0.2384 - accuracy: 0.5918 - val_loss: 0.3107 - val_accuracy: 0.6389\n",
            "Epoch 6/10\n",
            "525/525 [==============================] - 386s 736ms/step - loss: 0.1940 - accuracy: 0.6807 - val_loss: 0.2817 - val_accuracy: 0.7139\n",
            "Epoch 7/10\n",
            "525/525 [==============================] - 397s 756ms/step - loss: 0.1992 - accuracy: 0.6597 - val_loss: 0.2661 - val_accuracy: 0.7082\n",
            "Epoch 8/10\n",
            "525/525 [==============================] - 394s 750ms/step - loss: 0.1758 - accuracy: 0.6843 - val_loss: 0.2707 - val_accuracy: 0.6725\n",
            "Epoch 9/10\n",
            "525/525 [==============================] - 394s 751ms/step - loss: 0.1635 - accuracy: 0.7067 - val_loss: 0.2816 - val_accuracy: 0.7020\n",
            "Epoch 10/10\n",
            "525/525 [==============================] - 402s 764ms/step - loss: 0.1698 - accuracy: 0.6948 - val_loss: 0.2583 - val_accuracy: 0.7194\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7dad38294a00>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "y_pred_val = model.predict(X_val_padded)\n",
        "y_pred_val_binary = np.where(y_pred_val >= 0.5, 1, 0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JA8PRw5BXy_t",
        "outputId": "054862a8-4967-41b2-ffa1-e97c5029990e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "132/132 [==============================] - 17s 123ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_val, y_pred_val_binary)\n",
        "f1 = f1_score(y_val, y_pred_val_binary, average='weighted')\n",
        "precision = precision_score(y_val, y_pred_val_binary, average='weighted')\n",
        "recall = recall_score(y_val, y_pred_val_binary, average='weighted')\n",
        "hamming = hamming_loss(y_val, y_pred_val_binary)\n",
        "coverage = coverage_error(y_val, y_pred_val_binary)\n",
        "tn, fp, fn, tp = confusion_matrix(y_val.ravel(), y_pred_val_binary.ravel()).ravel()\n",
        "g_mean = np.sqrt((tp / (tp + fn)) * (tn / (tn + fp)))\n",
        "\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
        "print(\"F1 Score: {:.4f}\".format(f1))\n",
        "print(\"Precision: {:.4f}\".format(precision))\n",
        "print(\"Recall: {:.4f}\".format(recall))\n",
        "print(\"Hamming Loss: {:.4f}\".format(hamming))\n",
        "print(\"Coverage: {:.4f}\".format(coverage))\n",
        "print(\"G-Mean: {:.4f}\".format(g_mean))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8baptJYxX21l",
        "outputId": "a0f3027d-316e-4a9c-9848-2daccaf72acc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5802\n",
            "F1 Score: 0.7185\n",
            "Precision: 0.7927\n",
            "Recall: 0.6655\n",
            "Hamming Loss: 0.1066\n",
            "Coverage: 2.9385\n",
            "G-Mean: 0.7969\n"
          ]
        }
      ]
    }
  ]
}