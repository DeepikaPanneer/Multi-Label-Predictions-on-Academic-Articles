{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y9y-6jMlsMFt",
    "outputId": "81822704-359a-41c4-e384-d970283401af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Users/deepika/anaconda3/lib/python3.11/site-packages/pygcn-0.1-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-multilearn in /Users/deepika/anaconda3/lib/python3.11/site-packages (0.2.0)\n",
      "\u001b[33mDEPRECATION: Loading egg at /Users/deepika/anaconda3/lib/python3.11/site-packages/pygcn-0.1-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting scikit-learn-extra\n",
      "  Downloading scikit-learn-extra-0.3.0.tar.gz (818 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.0/819.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /Users/deepika/anaconda3/lib/python3.11/site-packages (from scikit-learn-extra) (1.24.3)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/deepika/anaconda3/lib/python3.11/site-packages (from scikit-learn-extra) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=0.23.0 in /Users/deepika/anaconda3/lib/python3.11/site-packages (from scikit-learn-extra) (1.4.1.post1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/deepika/anaconda3/lib/python3.11/site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/deepika/anaconda3/lib/python3.11/site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (2.2.0)\n",
      "Building wheels for collected packages: scikit-learn-extra\n",
      "  Building wheel for scikit-learn-extra (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for scikit-learn-extra: filename=scikit_learn_extra-0.3.0-cp311-cp311-macosx_11_0_arm64.whl size=388824 sha256=bef2fca0e5a85e4739148a55cf0b0408c831bf74dc4bce662566cd88a1513a90\n",
      "  Stored in directory: /Users/deepika/Library/Caches/pip/wheels/9c/cb/bd/4f19f79eee4fe83d1303d9be845bf9b07507dcac5439fa30ba\n",
      "Successfully built scikit-learn-extra\n",
      "Installing collected packages: scikit-learn-extra\n",
      "Successfully installed scikit-learn-extra-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-multilearn\n",
    "!pip install scikit-learn-extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HbeZCjcfVqFq",
    "outputId": "a98ccc16-dfc3-4269-8ef5-c30ecf679774"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ewDE7rkla4cy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, hamming_loss,coverage_error, confusion_matrix\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.metrics import coverage_error\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import random\n",
    "\n",
    "# Load the dataset\n",
    "#train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
    "train_data = pd.read_csv('data/train.csv')\n",
    "train_data['TEXT'] = train_data['TITLE'] + ' ' + train_data['ABSTRACT']\n",
    "\n",
    "X = train_data['TEXT']\n",
    "y = train_data[['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']].values\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Train a Word2Vec model\n",
    "# sentences = [text.split() for text in X_train]\n",
    "# word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Load the Google's pre-trained Word2Vec model\n",
    "word2vec_path = '/Users/deepika/Documents/Masters/Courses/Practicum/GoogleNews-vectors-negative300.bin'  # Provide the path to the downloaded Word2Vec binary file\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_vectors(data, model, num_features):\n",
    "    return np.array([average_word_vectors(words, model, num_features) for words in data])\n",
    "\n",
    "def average_word_vectors(words, model, num_features):\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            n_words += 1\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    if n_words:\n",
    "        feature_vector = np.divide(feature_vector, n_words)\n",
    "    return feature_vector\n",
    "\n",
    "# Replace '300' with the actual dimensionality of your word vectors\n",
    "X_train_word2vec = get_average_vectors([text.split() for text in X_train], word2vec_model, 300)\n",
    "X_val_word2vec = get_average_vectors([text.split() for text in X_val], word2vec_model, 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gDYKYmL8a7Fm"
   },
   "outputs": [],
   "source": [
    "# # Vectorize your text data using Word2Vec embeddings\n",
    "# def average_word_vectors(words, model, num_features):\n",
    "#     feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "#     n_words = 0\n",
    "#     for word in words:\n",
    "#         if word in model.wv:\n",
    "#             n_words += 1\n",
    "#             feature_vector = np.add(feature_vector, model.wv[word])\n",
    "#     if n_words:\n",
    "#         feature_vector = np.divide(feature_vector, n_words)\n",
    "#     return feature_vector\n",
    "\n",
    "# def get_average_vectors(data, model, num_features):\n",
    "#     return np.array([average_word_vectors(words, model, num_features) for words in data])\n",
    "\n",
    "# # Vectorize train and validation data\n",
    "# X_train_word2vec = get_average_vectors([text.split() for text in X_train], word2vec_model, 100)\n",
    "# X_val_word2vec = get_average_vectors([text.split() for text in X_val], word2vec_model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9PSZ9_zQa9aQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Helper function to identify minority labels\n",
    "def get_tail_labels(y):\n",
    "    tail_labels = [i for i in range(y.shape[1]) if np.sum(y[:, i]) < (y.shape[0] / 2)]\n",
    "    return tail_labels\n",
    "\n",
    "# Dynamic MLSMOTE function\n",
    "def dynamic_MLSMOTE(X, y, target_balance=4500):\n",
    "    n_neighbors = min(5, len(X) - 1)\n",
    "    neigh = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "    neigh.fit(X)\n",
    "    tail_labels = get_tail_labels(y)\n",
    "    synthetic_samples = []\n",
    "    synthetic_labels = []\n",
    "\n",
    "    for i in tail_labels:\n",
    "        current_count = np.sum(y[:, i])\n",
    "        n_samples = max(target_balance - current_count, 0)  # Calculate the number of samples to generate\n",
    "        target_indices = np.where(y[:, i] == 1)[0]\n",
    "\n",
    "        if len(target_indices) >= n_neighbors:\n",
    "            nn = neigh.kneighbors(X[target_indices], return_distance=False)\n",
    "            for _ in range(n_samples):\n",
    "                sample_index = random.choice(range(len(target_indices)))\n",
    "                nn_indices = nn[sample_index, 1:]\n",
    "                chosen_nn = random.choice(nn_indices)\n",
    "                step = np.random.rand()\n",
    "                synthetic_sample = X[target_indices[sample_index]] + step * (X[chosen_nn] - X[target_indices[sample_index]])\n",
    "                synthetic_samples.append(synthetic_sample)\n",
    "                synthetic_label = y[target_indices[sample_index]].copy()\n",
    "                synthetic_labels.append(synthetic_label)\n",
    "\n",
    "    if len(synthetic_samples) > 0:\n",
    "        X_synthetic = np.vstack(synthetic_samples)\n",
    "        y_synthetic = np.vstack(synthetic_labels)\n",
    "        X_balanced = np.vstack((X, X_synthetic))\n",
    "        y_balanced = np.vstack((y, y_synthetic))\n",
    "        return X_balanced, y_balanced\n",
    "    else:\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VVivb0pha_rS",
    "outputId": "f77e3098-6861-4547-e36a-6fc024e12075"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4160\n",
      "F1 Score: 0.5506\n",
      "Precision: 0.8857\n",
      "Recall: 0.4261\n",
      "Hamming Loss: 0.1312\n",
      "Coverage: 3.8975\n",
      "G-Mean: 0.6485\n"
     ]
    }
   ],
   "source": [
    "y_train_np = y_train\n",
    "target_balance = 4500\n",
    "X_balanced_word2vec, y_balanced = dynamic_MLSMOTE(X_train_word2vec, y_train_np, target_balance=target_balance)\n",
    "\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_balanced_word2vec, y_balanced)\n",
    "\n",
    "y_pred_val = rf_classifier.predict(X_val_word2vec)\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred_val)\n",
    "f1 = f1_score(y_val, y_pred_val, average='weighted')\n",
    "precision = precision_score(y_val, y_pred_val, average='weighted')\n",
    "recall = recall_score(y_val, y_pred_val, average='weighted')\n",
    "hamming = hamming_loss(y_val, y_pred_val)\n",
    "coverage = coverage_error(y_val, y_pred_val)\n",
    "tn, fp, fn, tp = confusion_matrix(y_val.ravel(), y_pred_val.ravel()).ravel()\n",
    "g_mean = np.sqrt((tp / (tp + fn)) * (tn / (tn + fp)))\n",
    "\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"F1 Score: {:.4f}\".format(f1))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Hamming Loss: {:.4f}\".format(hamming))\n",
    "print(\"Coverage: {:.4f}\".format(coverage))\n",
    "print(\"G-Mean: {:.4f}\".format(g_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfmc12h6bCIS",
    "outputId": "ac9e6f1f-be68-456d-d3e6-1c4155642a39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Relevance:\n",
      "Accuracy: 0.4877\n",
      "F1 Score: 0.6367\n",
      "Precision: 0.8536\n",
      "Recall: 0.5273\n",
      "Hamming Loss: 0.1178\n",
      "Coverage: 3.4546\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance, ClassifierChain, LabelPowerset\n",
    "# Binary Relevance\n",
    "classifier_br = BinaryRelevance(RandomForestClassifier())\n",
    "classifier_br.fit(X_balanced_word2vec, y_balanced)\n",
    "y_pred_val_br = classifier_br.predict(X_val_word2vec)\n",
    "accuracy_br = accuracy_score(y_val, y_pred_val_br)\n",
    "f1_br = f1_score(y_val, y_pred_val_br, average='weighted')\n",
    "precision_br = precision_score(y_val, y_pred_val_br, average='weighted')\n",
    "recall_br = recall_score(y_val, y_pred_val_br, average='weighted')\n",
    "hamming_br = hamming_loss(y_val, y_pred_val_br)\n",
    "coverage_br = coverage_error(y_val, y_pred_val_br.toarray())\n",
    "print(\"Binary Relevance:\")\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy_br))\n",
    "print(\"F1 Score: {:.4f}\".format(f1_br))\n",
    "print(\"Precision: {:.4f}\".format(precision_br))\n",
    "print(\"Recall: {:.4f}\".format(recall_br))\n",
    "print(\"Hamming Loss: {:.4f}\".format(hamming_br))\n",
    "print(\"Coverage: {:.4f}\".format(coverage_br))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xisUHmtIbERT",
    "outputId": "97ebb712-0a91-44aa-a3f8-acd118e7d13d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Chains:\n",
      "Accuracy: 0.5125\n",
      "F1 Score: 0.6506\n",
      "Precision: 0.8524\n",
      "Recall: 0.5460\n",
      "Hamming Loss: 0.1153\n",
      "Coverage: 3.3650\n"
     ]
    }
   ],
   "source": [
    "# Classifier Chains\n",
    "classifier_cc = ClassifierChain(RandomForestClassifier())\n",
    "classifier_cc.fit(X_balanced_word2vec, y_balanced)\n",
    "y_pred_val_cc = classifier_cc.predict(X_val_word2vec)\n",
    "accuracy_cc = accuracy_score(y_val, y_pred_val_cc)\n",
    "f1_cc = f1_score(y_val, y_pred_val_cc, average='weighted')\n",
    "precision_cc = precision_score(y_val, y_pred_val_cc, average='weighted')\n",
    "recall_cc = recall_score(y_val, y_pred_val_cc, average='weighted')\n",
    "hamming_cc = hamming_loss(y_val, y_pred_val_cc)\n",
    "coverage_cc = coverage_error(y_val, y_pred_val_cc.toarray())\n",
    "print(\"Classifier Chains:\")\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy_cc))\n",
    "print(\"F1 Score: {:.4f}\".format(f1_cc))\n",
    "print(\"Precision: {:.4f}\".format(precision_cc))\n",
    "print(\"Recall: {:.4f}\".format(recall_cc))\n",
    "print(\"Hamming Loss: {:.4f}\".format(hamming_cc))\n",
    "print(\"Coverage: {:.4f}\".format(coverage_cc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qzbHE57QbHOo",
    "outputId": "8c6c9591-5442-4103-d869-7b6869f31bf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Powerset:\n",
      "Accuracy: 0.5793\n",
      "F1 Score: 0.6678\n",
      "Precision: 0.7782\n",
      "Recall: 0.6363\n",
      "Hamming Loss: 0.1166\n",
      "Coverage: 3.0529\n"
     ]
    }
   ],
   "source": [
    "# Label Powerset\n",
    "classifier_lp = LabelPowerset(RandomForestClassifier())\n",
    "classifier_lp.fit(X_balanced_word2vec, y_balanced)\n",
    "y_pred_val_lp = classifier_lp.predict(X_val_word2vec)\n",
    "accuracy_lp = accuracy_score(y_val, y_pred_val_lp)\n",
    "f1_lp = f1_score(y_val, y_pred_val_lp, average='weighted')\n",
    "precision_lp = precision_score(y_val, y_pred_val_lp, average='weighted')\n",
    "recall_lp = recall_score(y_val, y_pred_val_lp, average='weighted')\n",
    "hamming_lp = hamming_loss(y_val, y_pred_val_lp)\n",
    "coverage_lp = coverage_error(y_val, y_pred_val_lp.toarray())\n",
    "print(\"Label Powerset:\")\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy_lp))\n",
    "print(\"F1 Score: {:.4f}\".format(f1_lp))\n",
    "print(\"Precision: {:.4f}\".format(precision_lp))\n",
    "print(\"Recall: {:.4f}\".format(recall_lp))\n",
    "print(\"Hamming Loss: {:.4f}\".format(hamming_lp))\n",
    "print(\"Coverage: {:.4f}\".format(coverage_lp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZG0QGuVI6tpt"
   },
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, hamming_loss,coverage_error, confusion_matrix\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.metrics import coverage_error\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before applying dynamic MLSMOTE:\n",
      "Computer Science: 6902\n",
      "Physics: 4787\n",
      "Mathematics: 4468\n",
      "Statistics: 4137\n",
      "Quantitative Biology: 465\n",
      "Quantitative Finance: 204\n",
      "\n",
      "\n",
      "Class distribution after applying dynamic MLSMOTE:\n",
      "Computer Science: 7558\n",
      "Physics: 4800\n",
      "Mathematics: 4600\n",
      "Statistics: 5757\n",
      "Quantitative Biology: 4567\n",
      "Quantitative Finance: 4530\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import random\n",
    "\n",
    "# Load the dataset\n",
    "#train_data = pd.read_csv('train.csv')\n",
    "train_data = pd.read_csv('data/train.csv')\n",
    "train_data['TEXT'] = train_data['TITLE'] + ' ' + train_data['ABSTRACT']\n",
    "\n",
    "X = train_data['TEXT']\n",
    "y = train_data[['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']].values\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train).toarray()\n",
    "X_val_tfidf = vectorizer.transform(X_val).toarray()\n",
    "\n",
    "# Helper function to identify minority labels\n",
    "def get_tail_labels(y):\n",
    "    tail_labels = [i for i in range(y.shape[1]) if np.sum(y[:, i]) < (y.shape[0] / 2)]\n",
    "    return tail_labels\n",
    "\n",
    "# class distribution before applying dynamic MLSMOTE\n",
    "print(\"Class distribution before applying dynamic MLSMOTE:\")\n",
    "for i, label in enumerate(['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']):\n",
    "    print(f\"{label}: {np.sum(y_train[:, i])}\")\n",
    "\n",
    "# Dynamic MLSMOTE function\n",
    "def dynamic_MLSMOTE(X, y, target_balance=4500):\n",
    "    n_neighbors = min(5, len(X) - 1)\n",
    "    neigh = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "    neigh.fit(X)\n",
    "    tail_labels = get_tail_labels(y)\n",
    "    synthetic_samples = []\n",
    "    synthetic_labels = []\n",
    "\n",
    "    for i in tail_labels:\n",
    "        current_count = np.sum(y[:, i])\n",
    "        n_samples = max(target_balance - current_count, 0)  # Calculate the number of samples to generate\n",
    "        target_indices = np.where(y[:, i] == 1)[0]\n",
    "        \n",
    "        if len(target_indices) >= n_neighbors:\n",
    "            nn = neigh.kneighbors(X[target_indices], return_distance=False)\n",
    "            for _ in range(n_samples):\n",
    "                sample_index = random.choice(range(len(target_indices)))\n",
    "                nn_indices = nn[sample_index, 1:]\n",
    "                chosen_nn = random.choice(nn_indices)\n",
    "                step = np.random.rand()\n",
    "                synthetic_sample = X[target_indices[sample_index]] + step * (X[chosen_nn] - X[target_indices[sample_index]])\n",
    "                synthetic_samples.append(synthetic_sample)\n",
    "                synthetic_label = y[target_indices[sample_index]].copy()\n",
    "                synthetic_labels.append(synthetic_label)\n",
    "    \n",
    "    if len(synthetic_samples) > 0:\n",
    "        X_synthetic = np.vstack(synthetic_samples)\n",
    "        y_synthetic = np.vstack(synthetic_labels)\n",
    "        X_balanced = np.vstack((X, X_synthetic))\n",
    "        y_balanced = np.vstack((y, y_synthetic))\n",
    "        return X_balanced, y_balanced\n",
    "    else:\n",
    "        return X, y\n",
    "\n",
    "# Convert y_train to numpy array for processing\n",
    "y_train_np = y_train\n",
    "\n",
    "# Adjust this target balance\n",
    "target_balance = 4500  \n",
    "X_balanced_tfidf, y_balanced = dynamic_MLSMOTE(X_train_tfidf, y_train_np, target_balance=target_balance)\n",
    "\n",
    "# class distribution after applying dynamic MLSMOTE\n",
    "print(\"\\n\")\n",
    "print(\"Class distribution after applying dynamic MLSMOTE:\")\n",
    "for i, label in enumerate(['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']):\n",
    "    print(f\"{label}: {np.sum(y_balanced[:, i])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4844\n",
      "F1 Score: 0.6328\n",
      "Precision: 0.8582\n",
      "Recall: 0.5260\n",
      "Hamming Loss: 0.1162\n",
      "Coverage: 3.5099\n",
      "G-Mean: 0.7177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deepika/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_balanced_tfidf, y_balanced)\n",
    "\n",
    "y_pred_val = rf_classifier.predict(X_val_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred_val)\n",
    "f1 = f1_score(y_val, y_pred_val, average='weighted')\n",
    "precision = precision_score(y_val, y_pred_val, average='weighted')\n",
    "recall = recall_score(y_val, y_pred_val, average='weighted')\n",
    "hamming = hamming_loss(y_val, y_pred_val)\n",
    "coverage = coverage_error(y_val, y_pred_val)\n",
    "tn, fp, fn, tp = confusion_matrix(y_val.ravel(), y_pred_val.ravel()).ravel()\n",
    "g_mean = np.sqrt((tp / (tp + fn)) * (tn / (tn + fp)))\n",
    "\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"F1 Score: {:.4f}\".format(f1))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Hamming Loss: {:.4f}\".format(hamming))\n",
    "print(\"Coverage: {:.4f}\".format(coverage))\n",
    "print(\"G-Mean: {:.4f}\".format(g_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Relevance:\n",
      "Accuracy: 0.5657\n",
      "F1 Score: 0.7341\n",
      "Precision: 0.8720\n",
      "Recall: 0.6497\n",
      "Hamming Loss: 0.0941\n",
      "Coverage: 2.9971\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance, ClassifierChain, LabelPowerset\n",
    "# Binary Relevance\n",
    "classifier_br = BinaryRelevance(RandomForestClassifier())\n",
    "classifier_br.fit(X_balanced_tfidf, y_balanced)\n",
    "y_pred_val_br = classifier_br.predict(X_val_tfidf)\n",
    "accuracy_br = accuracy_score(y_val, y_pred_val_br)\n",
    "f1_br = f1_score(y_val, y_pred_val_br, average='weighted')\n",
    "precision_br = precision_score(y_val, y_pred_val_br, average='weighted')\n",
    "recall_br = recall_score(y_val, y_pred_val_br, average='weighted')\n",
    "hamming_br = hamming_loss(y_val, y_pred_val_br)\n",
    "coverage_br = coverage_error(y_val, y_pred_val_br.toarray())\n",
    "print(\"Binary Relevance:\")\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy_br))\n",
    "print(\"F1 Score: {:.4f}\".format(f1_br))\n",
    "print(\"Precision: {:.4f}\".format(precision_br))\n",
    "print(\"Recall: {:.4f}\".format(recall_br))\n",
    "print(\"Hamming Loss: {:.4f}\".format(hamming_br))\n",
    "print(\"Coverage: {:.4f}\".format(coverage_br))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Chains:\n",
      "Accuracy: 0.5847\n",
      "F1 Score: 0.7395\n",
      "Precision: 0.8666\n",
      "Recall: 0.6587\n",
      "Hamming Loss: 0.0928\n",
      "Coverage: 2.9359\n"
     ]
    }
   ],
   "source": [
    "# Classifier Chains\n",
    "classifier_cc = ClassifierChain(RandomForestClassifier())\n",
    "classifier_cc.fit(X_balanced_tfidf, y_balanced)\n",
    "y_pred_val_cc = classifier_cc.predict(X_val_tfidf)\n",
    "accuracy_cc = accuracy_score(y_val, y_pred_val_cc)\n",
    "f1_cc = f1_score(y_val, y_pred_val_cc, average='weighted')\n",
    "precision_cc = precision_score(y_val, y_pred_val_cc, average='weighted')\n",
    "recall_cc = recall_score(y_val, y_pred_val_cc, average='weighted')\n",
    "hamming_cc = hamming_loss(y_val, y_pred_val_cc)\n",
    "coverage_cc = coverage_error(y_val, y_pred_val_cc.toarray())\n",
    "print(\"Classifier Chains:\")\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy_cc))\n",
    "print(\"F1 Score: {:.4f}\".format(f1_cc))\n",
    "print(\"Precision: {:.4f}\".format(precision_cc))\n",
    "print(\"Recall: {:.4f}\".format(recall_cc))\n",
    "print(\"Hamming Loss: {:.4f}\".format(hamming_cc))\n",
    "print(\"Coverage: {:.4f}\".format(coverage_cc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Powerset:\n",
      "Accuracy: 0.6069\n",
      "F1 Score: 0.6901\n",
      "Precision: 0.8023\n",
      "Recall: 0.6652\n",
      "Hamming Loss: 0.1068\n",
      "Coverage: 2.9070\n"
     ]
    }
   ],
   "source": [
    "# Label Powerset\n",
    "classifier_lp = LabelPowerset(RandomForestClassifier())\n",
    "classifier_lp.fit(X_balanced_tfidf, y_balanced)\n",
    "y_pred_val_lp = classifier_lp.predict(X_val_tfidf)\n",
    "accuracy_lp = accuracy_score(y_val, y_pred_val_lp)\n",
    "f1_lp = f1_score(y_val, y_pred_val_lp, average='weighted')\n",
    "precision_lp = precision_score(y_val, y_pred_val_lp, average='weighted')\n",
    "recall_lp = recall_score(y_val, y_pred_val_lp, average='weighted')\n",
    "hamming_lp = hamming_loss(y_val, y_pred_val_lp)\n",
    "coverage_lp = coverage_error(y_val, y_pred_val_lp.toarray())\n",
    "print(\"Label Powerset:\")\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy_lp))\n",
    "print(\"F1 Score: {:.4f}\".format(f1_lp))\n",
    "print(\"Precision: {:.4f}\".format(precision_lp))\n",
    "print(\"Recall: {:.4f}\".format(recall_lp))\n",
    "print(\"Hamming Loss: {:.4f}\".format(hamming_lp))\n",
    "print(\"Coverage: {:.4f}\".format(coverage_lp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
