{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D1UrM4fqhia",
        "outputId": "fdb3ec87-aafb-40c6-94d2-3e2a7d2ec975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-multilearn\n",
            "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n",
            "Collecting scikit-learn-extra\n",
            "  Downloading scikit_learn_extra-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.3.0)\n",
            "Installing collected packages: scikit-learn-extra\n",
            "Successfully installed scikit-learn-extra-0.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-multilearn\n",
        "!pip install scikit-learn-extra"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmqWd5pXqyMZ",
        "outputId": "9a08e085-f793-4bed-f4a0-3460a8473d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, hamming_loss,coverage_error, confusion_matrix\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.metrics import coverage_error\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from gensim.models import Word2Vec\n",
        "import random"
      ],
      "metadata": {
        "id": "e77S5D5hqxDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/practicum/data.csv')"
      ],
      "metadata": {
        "id": "NQvUNozHrLeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/practicum/data.csv')\n",
        "train_data['TEXT'] = train_data['TITLE'] + ' ' + train_data['ABSTRACT']\n",
        "\n",
        "X = train_data['TEXT']\n",
        "y = train_data[['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']].values\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Word2Vec model\n",
        "sentences = [text.split() for text in X_train]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "WBpT6yw5q2Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize your text data using Word2Vec embeddings\n",
        "def average_word_vectors(words, model, num_features):\n",
        "    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
        "    n_words = 0\n",
        "    for word in words:\n",
        "        if word in model.wv:\n",
        "            n_words += 1\n",
        "            feature_vector = np.add(feature_vector, model.wv[word])\n",
        "    if n_words:\n",
        "        feature_vector = np.divide(feature_vector, n_words)\n",
        "    return feature_vector\n",
        "\n",
        "def get_average_vectors(data, model, num_features):\n",
        "    return np.array([average_word_vectors(words, model, num_features) for words in data])\n",
        "\n",
        "# Vectorize train and validation data\n",
        "X_train_word2vec = get_average_vectors([text.split() for text in X_train], word2vec_model, 100)\n",
        "X_val_word2vec = get_average_vectors([text.split() for text in X_val], word2vec_model, 100)"
      ],
      "metadata": {
        "id": "Iy35JvK0rSH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to identify minority labels\n",
        "def get_tail_labels(y):\n",
        "    tail_labels = [i for i in range(y.shape[1]) if np.sum(y[:, i]) < (y.shape[0] / 2)]\n",
        "    return tail_labels\n",
        "\n",
        "# Dynamic MLSMOTE function\n",
        "def dynamic_MLSMOTE(X, y, target_balance=4500):\n",
        "    n_neighbors = min(5, len(X) - 1)\n",
        "    neigh = NearestNeighbors(n_neighbors=n_neighbors)\n",
        "    neigh.fit(X)\n",
        "    tail_labels = get_tail_labels(y)\n",
        "    synthetic_samples = []\n",
        "    synthetic_labels = []\n",
        "\n",
        "    for i in tail_labels:\n",
        "        current_count = np.sum(y[:, i])\n",
        "        n_samples = max(target_balance - current_count, 0)  # Calculate the number of samples to generate\n",
        "        target_indices = np.where(y[:, i] == 1)[0]\n",
        "\n",
        "        if len(target_indices) >= n_neighbors:\n",
        "            nn = neigh.kneighbors(X[target_indices], return_distance=False)\n",
        "            for _ in range(n_samples):\n",
        "                sample_index = random.choice(range(len(target_indices)))\n",
        "                nn_indices = nn[sample_index, 1:]\n",
        "                chosen_nn = random.choice(nn_indices)\n",
        "                step = np.random.rand()\n",
        "                synthetic_sample = X[target_indices[sample_index]] + step * (X[chosen_nn] - X[target_indices[sample_index]])\n",
        "                synthetic_samples.append(synthetic_sample)\n",
        "                synthetic_label = y[target_indices[sample_index]].copy()\n",
        "                synthetic_labels.append(synthetic_label)\n",
        "\n",
        "    if len(synthetic_samples) > 0:\n",
        "        X_synthetic = np.vstack(synthetic_samples)\n",
        "        y_synthetic = np.vstack(synthetic_labels)\n",
        "        X_balanced = np.vstack((X, X_synthetic))\n",
        "        y_balanced = np.vstack((y, y_synthetic))\n",
        "        return X_balanced, y_balanced\n",
        "    else:\n",
        "        return X, y"
      ],
      "metadata": {
        "id": "0fVGxyUCra0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zGiaeOABrh_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVivb0pha_rS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7c1be80-1af8-4016-8410-79f9ae8d22ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6186\n",
            "F1 Score: 0.7721\n",
            "Precision: 0.8563\n",
            "Recall: 0.7074\n",
            "Hamming Loss: 0.0863\n",
            "Coverage: 2.7449\n",
            "G-Mean: 0.8278\n"
          ]
        }
      ],
      "source": [
        "y_train_np = y_train\n",
        "target_balance = 4500\n",
        "X_balanced_word2vec, y_balanced = dynamic_MLSMOTE(X_train_word2vec, y_train_np, target_balance=target_balance)\n",
        "\n",
        "\n",
        "rf_classifier = RandomForestClassifier()\n",
        "rf_classifier.fit(X_balanced_word2vec, y_balanced)\n",
        "\n",
        "y_pred_val = rf_classifier.predict(X_val_word2vec)\n",
        "\n",
        "accuracy = accuracy_score(y_val, y_pred_val)\n",
        "f1 = f1_score(y_val, y_pred_val, average='weighted')\n",
        "precision = precision_score(y_val, y_pred_val, average='weighted')\n",
        "recall = recall_score(y_val, y_pred_val, average='weighted')\n",
        "hamming = hamming_loss(y_val, y_pred_val)\n",
        "coverage = coverage_error(y_val, y_pred_val)\n",
        "tn, fp, fn, tp = confusion_matrix(y_val.ravel(), y_pred_val.ravel()).ravel()\n",
        "g_mean = np.sqrt((tp / (tp + fn)) * (tn / (tn + fp)))\n",
        "\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
        "print(\"F1 Score: {:.4f}\".format(f1))\n",
        "print(\"Precision: {:.4f}\".format(precision))\n",
        "print(\"Recall: {:.4f}\".format(recall))\n",
        "print(\"Hamming Loss: {:.4f}\".format(hamming))\n",
        "print(\"Coverage: {:.4f}\".format(coverage))\n",
        "print(\"G-Mean: {:.4f}\".format(g_mean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfmc12h6bCIS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3525eebd-99bb-4efa-e6c7-4e8b3e80a29e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Relevance:\n",
            "Accuracy: 0.6260\n",
            "F1 Score: 0.7784\n",
            "Precision: 0.8452\n",
            "Recall: 0.7251\n",
            "Hamming Loss: 0.0854\n",
            "Coverage: 2.6684\n"
          ]
        }
      ],
      "source": [
        "from skmultilearn.problem_transform import BinaryRelevance, ClassifierChain, LabelPowerset\n",
        "# Binary Relevance\n",
        "classifier_br = BinaryRelevance(RandomForestClassifier())\n",
        "classifier_br.fit(X_balanced_word2vec, y_balanced)\n",
        "y_pred_val_br = classifier_br.predict(X_val_word2vec)\n",
        "accuracy_br = accuracy_score(y_val, y_pred_val_br)\n",
        "f1_br = f1_score(y_val, y_pred_val_br, average='weighted')\n",
        "precision_br = precision_score(y_val, y_pred_val_br, average='weighted')\n",
        "recall_br = recall_score(y_val, y_pred_val_br, average='weighted')\n",
        "hamming_br = hamming_loss(y_val, y_pred_val_br)\n",
        "coverage_br = coverage_error(y_val, y_pred_val_br.toarray())\n",
        "print(\"Binary Relevance:\")\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy_br))\n",
        "print(\"F1 Score: {:.4f}\".format(f1_br))\n",
        "print(\"Precision: {:.4f}\".format(precision_br))\n",
        "print(\"Recall: {:.4f}\".format(recall_br))\n",
        "print(\"Hamming Loss: {:.4f}\".format(hamming_br))\n",
        "print(\"Coverage: {:.4f}\".format(coverage_br))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xisUHmtIbERT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "477c259f-add3-423d-f066-e15accfdc4c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier Chains:\n",
            "Accuracy: 0.6303\n",
            "F1 Score: 0.7808\n",
            "Precision: 0.8441\n",
            "Recall: 0.7281\n",
            "Hamming Loss: 0.0863\n",
            "Coverage: 2.6479\n"
          ]
        }
      ],
      "source": [
        "# Classifier Chains\n",
        "classifier_cc = ClassifierChain(RandomForestClassifier())\n",
        "classifier_cc.fit(X_balanced_word2vec, y_balanced)\n",
        "y_pred_val_cc = classifier_cc.predict(X_val_word2vec)\n",
        "accuracy_cc = accuracy_score(y_val, y_pred_val_cc)\n",
        "f1_cc = f1_score(y_val, y_pred_val_cc, average='weighted')\n",
        "precision_cc = precision_score(y_val, y_pred_val_cc, average='weighted')\n",
        "recall_cc = recall_score(y_val, y_pred_val_cc, average='weighted')\n",
        "hamming_cc = hamming_loss(y_val, y_pred_val_cc)\n",
        "coverage_cc = coverage_error(y_val, y_pred_val_cc.toarray())\n",
        "print(\"Classifier Chains:\")\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy_cc))\n",
        "print(\"F1 Score: {:.4f}\".format(f1_cc))\n",
        "print(\"Precision: {:.4f}\".format(precision_cc))\n",
        "print(\"Recall: {:.4f}\".format(recall_cc))\n",
        "print(\"Hamming Loss: {:.4f}\".format(hamming_cc))\n",
        "print(\"Coverage: {:.4f}\".format(coverage_cc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzbHE57QbHOo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8805a3d-79c8-4a7e-8a8c-a7aeb4ad8531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Powerset:\n",
            "Accuracy: 0.6627\n",
            "F1 Score: 0.7834\n",
            "Precision: 0.8332\n",
            "Recall: 0.7466\n",
            "Hamming Loss: 0.0878\n",
            "Coverage: 2.5552\n"
          ]
        }
      ],
      "source": [
        "# Label Powerset\n",
        "classifier_lp = LabelPowerset(RandomForestClassifier())\n",
        "classifier_lp.fit(X_balanced_word2vec, y_balanced)\n",
        "y_pred_val_lp = classifier_lp.predict(X_val_word2vec)\n",
        "accuracy_lp = accuracy_score(y_val, y_pred_val_lp)\n",
        "f1_lp = f1_score(y_val, y_pred_val_lp, average='weighted')\n",
        "precision_lp = precision_score(y_val, y_pred_val_lp, average='weighted')\n",
        "recall_lp = recall_score(y_val, y_pred_val_lp, average='weighted')\n",
        "hamming_lp = hamming_loss(y_val, y_pred_val_lp)\n",
        "coverage_lp = coverage_error(y_val, y_pred_val_lp.toarray())\n",
        "print(\"Label Powerset:\")\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy_lp))\n",
        "print(\"F1 Score: {:.4f}\".format(f1_lp))\n",
        "print(\"Precision: {:.4f}\".format(precision_lp))\n",
        "print(\"Recall: {:.4f}\".format(recall_lp))\n",
        "print(\"Hamming Loss: {:.4f}\".format(hamming_lp))\n",
        "print(\"Coverage: {:.4f}\".format(coverage_lp))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "zQcfnTdEsLFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Powerset SVM\n",
        "classifier_lp = LabelPowerset(SVC(class_weight='balanced'))\n",
        "classifier_lp.fit(X_balanced_word2vec, y_balanced)\n",
        "y_pred_val_lp = classifier_lp.predict(X_val_word2vec)\n",
        "accuracy_lp = accuracy_score(y_val, y_pred_val_lp)\n",
        "f1_lp = f1_score(y_val, y_pred_val_lp, average='weighted')\n",
        "precision_lp = precision_score(y_val, y_pred_val_lp, average='weighted')\n",
        "recall_lp = recall_score(y_val, y_pred_val_lp, average='weighted')\n",
        "hamming_lp = hamming_loss(y_val, y_pred_val_lp)\n",
        "coverage_lp = coverage_error(y_val, y_pred_val_lp.toarray())\n",
        "print(\"Label Powerset:\")\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy_lp))\n",
        "print(\"F1 Score: {:.4f}\".format(f1_lp))\n",
        "print(\"Precision: {:.4f}\".format(precision_lp))\n",
        "print(\"Recall: {:.4f}\".format(recall_lp))\n",
        "print(\"Hamming Loss: {:.4f}\".format(hamming_lp))\n",
        "print(\"Coverage: {:.4f}\".format(coverage_lp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhgCBK-crm88",
        "outputId": "77b2c2a8-cb35-45c7-ced8-c983a24ca85c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Powerset:\n",
            "Accuracy: 0.5521\n",
            "F1 Score: 0.7787\n",
            "Precision: 0.7245\n",
            "Recall: 0.8501\n",
            "Hamming Loss: 0.1078\n",
            "Coverage: 2.3130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary Relevance SVM\n",
        "classifier_br = BinaryRelevance(SVC(class_weight='balanced'))\n",
        "classifier_br.fit(X_balanced_word2vec, y_balanced)\n",
        "y_pred_val_br = classifier_br.predict(X_val_word2vec)\n",
        "accuracy_br = accuracy_score(y_val, y_pred_val_br)\n",
        "f1_br = f1_score(y_val, y_pred_val_br, average='weighted')\n",
        "precision_br = precision_score(y_val, y_pred_val_br, average='weighted')\n",
        "recall_br = recall_score(y_val, y_pred_val_br, average='weighted')\n",
        "hamming_br = hamming_loss(y_val, y_pred_val_br)\n",
        "coverage_br = coverage_error(y_val, y_pred_val_br.toarray())\n",
        "print(\"Binary Relevance:\")\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy_br))\n",
        "print(\"F1 Score: {:.4f}\".format(f1_br))\n",
        "print(\"Precision: {:.4f}\".format(precision_br))\n",
        "print(\"Recall: {:.4f}\".format(recall_br))\n",
        "print(\"Hamming Loss: {:.4f}\".format(hamming_br))\n",
        "print(\"Coverage: {:.4f}\".format(coverage_br))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzCWTvbzsBAC",
        "outputId": "5da553cd-2079-4c80-b2ca-6ee461af1377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Relevance:\n",
            "Accuracy: 0.5232\n",
            "F1 Score: 0.7988\n",
            "Precision: 0.7399\n",
            "Recall: 0.8859\n",
            "Hamming Loss: 0.1130\n",
            "Coverage: 2.2400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary Relevance SVM\n",
        "# classifier_br = BinaryRelevance()\n",
        "classifier_cc = ClassifierChain(SVC(class_weight='balanced'))\n",
        "classifier_cc.fit(X_balanced_word2vec, y_balanced)\n",
        "y_pred_val_cc = classifier_cc.predict(X_val_word2vec)\n",
        "accuracy_cc = accuracy_score(y_val, y_pred_val_cc)\n",
        "f1_cc = f1_score(y_val, y_pred_val_cc, average='weighted')\n",
        "precision_cc = precision_score(y_val, y_pred_val_cc, average='weighted')\n",
        "recall_cc = recall_score(y_val, y_pred_val_cc, average='weighted')\n",
        "hamming_cc = hamming_loss(y_val, y_pred_val_cc)\n",
        "# coverage_cc = coverage_error(y_val, y_pred_val_cc.toarray())\n",
        "print(\"Classifier Chains:\")\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy_cc))\n",
        "print(\"F1 Score: {:.4f}\".format(f1_cc))\n",
        "print(\"Precision: {:.4f}\".format(precision_cc))\n",
        "print(\"Recall: {:.4f}\".format(recall_cc))\n",
        "print(\"Hamming Loss: {:.4f}\".format(hamming_cc))\n",
        "# print(\"Coverage: {:.4f}\".format(coverage_cc))"
      ],
      "metadata": {
        "id": "8Sz7Igm7sYsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classifier Chains:\")\n",
        "coverage_cc = coverage_error(y_val, y_pred_val_cc)\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy_cc))\n",
        "print(\"F1 Score: {:.4f}\".format(f1_cc))\n",
        "print(\"Precision: {:.4f}\".format(precision_cc))\n",
        "print(\"Recall: {:.4f}\".format(recall_cc))\n",
        "print(\"Hamming Loss: {:.4f}\".format(hamming_cc))\n",
        "print(\"Coverage: {:.4f}\".format(coverage_cc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsGVUEzH0cje",
        "outputId": "30f1102c-7ab9-44d1-c8f9-36fa6ce6e5f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier Chains:\n",
            "Accuracy: 0.5998\n",
            "F1 Score: 0.8050\n",
            "Precision: 0.7531\n",
            "Recall: 0.8695\n",
            "Hamming Loss: 0.0922\n",
            "Coverage: 2.1881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HAoQ28LeB1Kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# from skmultilearn.adapt import MLkNN\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# # parameters = {'k': range(1,3), 's': [0.5, 0.7, 1.0]}\n",
        "\n",
        "# clf = MLkNN(k=10)\n",
        "# clf.fit(X_balanced_word2vec, y_balanced)\n",
        "\n",
        "# print (clf.best_params_, clf.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "lsM6dXWpE79V",
        "outputId": "c62c9828-0cc8-498f-97f5-6ca803c82e2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "NearestNeighbors.__init__() takes 1 positional argument but 2 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-05035f196a1e>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLkNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_balanced_word2vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_balanced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skmultilearn/adapt/mlknn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prior_prob_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prior_prob_false\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_label_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# Computing the posterior probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond_prob_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond_prob_false\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_label_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skmultilearn/adapt/mlknn.py\u001b[0m in \u001b[0;36m_compute_cond\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \"\"\"\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mknn_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlil_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'i8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mcn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlil_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'i8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: NearestNeighbors.__init__() takes 1 positional argument but 2 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FPG0ALwEG034"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}