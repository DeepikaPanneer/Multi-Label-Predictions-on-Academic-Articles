{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e56415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import random\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv('data/train.csv')\n",
    "train_data['TEXT'] = train_data['TITLE'] + ' ' + train_data['ABSTRACT']\n",
    "\n",
    "X = train_data['TEXT']\n",
    "y = train_data[['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']].values\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess text data for Word2Vec\n",
    "X_train_preprocessed = [text.split() for text in X_train]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=X_train_preprocessed, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to convert text to word embeddings\n",
    "def text_to_wv(text, model):\n",
    "    vectors = []\n",
    "    for word in text.split():\n",
    "        if word in model.wv:\n",
    "            vectors.append(model.wv[word])\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Convert text data to word embeddings\n",
    "X_train_wv = np.array([text_to_wv(text, word2vec_model) for text in X_train])\n",
    "X_val_wv = np.array([text_to_wv(text, word2vec_model) for text in X_val])\n",
    "\n",
    "# Helper function to identify minority labels\n",
    "def get_tail_labels(y):\n",
    "    tail_labels = [i for i in range(y.shape[1]) if np.sum(y[:, i]) < (y.shape[0] / 2)]\n",
    "    return tail_labels\n",
    "\n",
    "# class distribution before applying dynamic MLSMOTE\n",
    "print(\"Class distribution before applying dynamic MLSMOTE:\")\n",
    "for i, label in enumerate(['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']):\n",
    "    print(f\"{label}: {np.sum(y_train[:, i])}\")\n",
    "\n",
    "# Dynamic MLSMOTE function\n",
    "def dynamic_MLSMOTE(X, y, target_balance=4500):\n",
    "    n_neighbors = min(5, len(X) - 1)\n",
    "    neigh = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "    neigh.fit(X)\n",
    "    tail_labels = get_tail_labels(y)\n",
    "    synthetic_samples = []\n",
    "    synthetic_labels = []\n",
    "\n",
    "    for i in tail_labels:\n",
    "        current_count = np.sum(y[:, i])\n",
    "        n_samples = max(target_balance - current_count, 0)  # Calculate the number of samples to generate\n",
    "        target_indices = np.where(y[:, i] == 1)[0]\n",
    "        \n",
    "        if len(target_indices) >= n_neighbors:\n",
    "            nn = neigh.kneighbors(X[target_indices], return_distance=False)\n",
    "            for _ in range(n_samples):\n",
    "                sample_index = random.choice(range(len(target_indices)))\n",
    "                nn_indices = nn[sample_index, 1:]\n",
    "                chosen_nn = random.choice(nn_indices)\n",
    "                step = np.random.rand()\n",
    "                synthetic_sample = X[target_indices[sample_index]] + step * (X[chosen_nn] - X[target_indices[sample_index]])\n",
    "                synthetic_samples.append(synthetic_sample)\n",
    "                synthetic_label = y[target_indices[sample_index]].copy()\n",
    "                synthetic_labels.append(synthetic_label)\n",
    "    \n",
    "    if len(synthetic_samples) > 0:\n",
    "        X_synthetic = np.vstack(synthetic_samples)\n",
    "        y_synthetic = np.vstack(synthetic_labels)\n",
    "        X_balanced = np.vstack((X, X_synthetic))\n",
    "        y_balanced = np.vstack((y, y_synthetic))\n",
    "        return X_balanced, y_balanced\n",
    "    else:\n",
    "        return X, y\n",
    "\n",
    "# Convert y_train to numpy array for processing\n",
    "y_train_np = y_train\n",
    "\n",
    "# Adjust this target balance\n",
    "target_balance = 4500  \n",
    "X_balanced_wv, y_balanced = dynamic_MLSMOTE(X_train_wv, y_train_np, target_balance=target_balance)\n",
    "\n",
    "# class distribution after applying dynamic MLSMOTE\n",
    "print(\"\\n\")\n",
    "print(\"Class distribution after applying dynamic MLSMOTE:\")\n",
    "for i, label in enumerate(['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']):\n",
    "    print(f\"{label}: {np.sum(y_balanced[:, i])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, f1_score\n",
    "from skmultilearn.problem_transform import BinaryRelevance, LabelPowerset, ClassifierChain\n",
    "import numpy as np\n",
    "\n",
    "# Define the additional evaluation metrics functions\n",
    "def geometric_mean_score(y_true, y_pred):\n",
    "    gmean = np.sqrt(accuracy_score(y_true, y_pred, normalize=True) *\n",
    "                     accuracy_score(y_true, y_pred, normalize=True))\n",
    "    return gmean\n",
    "\n",
    "def balanced_accuracy_score(y_true, y_pred):\n",
    "    return balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "def one_error(y_true, y_pred):\n",
    "    incorrect = np.sum(np.logical_and((y_true == 0), (y_pred == 1)))\n",
    "    one_err = incorrect / y_true.shape[0]\n",
    "    return one_err\n",
    "\n",
    "\n",
    "def coverage_error(y_true, y_pred):\n",
    "    cov_err = np.mean(np.sum(y_pred, axis=1))\n",
    "    return cov_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d4ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base classifier\n",
    "base_classifier = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Binary Relevance\n",
    "binary_relevance_classifier = BinaryRelevance(classifier=base_classifier)\n",
    "binary_relevance_classifier.fit(X_balanced_wv, y_balanced)\n",
    "y_pred_binary_relevance = binary_relevance_classifier.predict(X_val_wv)\n",
    "\n",
    "# Label Powerset\n",
    "label_powerset_classifier = LabelPowerset(classifier=base_classifier)\n",
    "label_powerset_classifier.fit(X_balanced_wv, y_balanced)\n",
    "y_pred_label_powerset = label_powerset_classifier.predict(X_val_wv)\n",
    "\n",
    "# Classifier Chains\n",
    "classifier_chain_classifier = ClassifierChain(classifier=base_classifier)\n",
    "classifier_chain_classifier.fit(X_balanced_wv, y_balanced)\n",
    "y_pred_classifier_chain = classifier_chain_classifier.predict(X_val_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0970937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics for Binary Relevance\n",
    "accuracy_binary_relevance = accuracy_score(y_val, y_pred_binary_relevance)\n",
    "hamming_loss_binary_relevance = hamming_loss(y_val, y_pred_binary_relevance)\n",
    "f1_binary_relevance = f1_score(y_val, y_pred_binary_relevance, average='micro')\n",
    "gmean_binary_relevance = geometric_mean_score(y_val, y_pred_binary_relevance)\n",
    "one_error_binary_relevance = one_error(y_val, y_pred_binary_relevance)\n",
    "coverage_binary_relevance = coverage_error(y_val, y_pred_binary_relevance)\n",
    "\n",
    "# Calculate evaluation metrics for Label Powerset\n",
    "accuracy_label_powerset = accuracy_score(y_val, y_pred_label_powerset)\n",
    "hamming_loss_label_powerset = hamming_loss(y_val, y_pred_label_powerset)\n",
    "f1_label_powerset = f1_score(y_val, y_pred_label_powerset, average='micro')\n",
    "gmean_label_powerset = geometric_mean_score(y_val, y_pred_label_powerset)\n",
    "one_error_label_powerset = one_error(y_val, y_pred_label_powerset)\n",
    "coverage_label_powerset = coverage_error(y_val, y_pred_label_powerset)\n",
    "\n",
    "# Calculate evaluation metrics for Classifier Chains\n",
    "accuracy_classifier_chain = accuracy_score(y_val, y_pred_classifier_chain)\n",
    "hamming_loss_classifier_chain = hamming_loss(y_val, y_pred_classifier_chain)\n",
    "f1_classifier_chain = f1_score(y_val, y_pred_classifier_chain, average='micro')\n",
    "gmean_classifier_chain = geometric_mean_score(y_val, y_pred_classifier_chain)\n",
    "one_error_classifier_chain = one_error(y_val, y_pred_classifier_chain)\n",
    "coverage_classifier_chain = coverage_error(y_val, y_pred_classifier_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc81df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking_loss(y_true, y_pred): \n",
    "    loss = 0\n",
    "    for i in range(len(y_true)):\n",
    "        diff = np.sum(np.abs(y_true[i] - y_pred[i]))\n",
    "        loss += diff / (len(y_true[i]) * (len(y_true[i]) - 1))\n",
    "    return loss / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1b8405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate evaluation metrics for Binary Relevance\n",
    "accuracy_binary_relevance = accuracy_score(pd.DataFrame(y_val), y_pred_binary_relevance)\n",
    "precision_binary_relevance = precision_score(pd.DataFrame(y_val), y_pred_binary_relevance, average='micro')\n",
    "recall_binary_relevance = recall_score(pd.DataFrame(y_val), y_pred_binary_relevance, average='micro')\n",
    "f1_binary_relevance = f1_score(pd.DataFrame(y_val), y_pred_binary_relevance, average='micro')\n",
    "\n",
    "# Calculate evaluation metrics for Label Powerset\n",
    "accuracy_label_powerset = accuracy_score(pd.DataFrame(y_val), y_pred_label_powerset)\n",
    "precision_label_powerset = precision_score(pd.DataFrame(y_val), y_pred_label_powerset, average='micro')\n",
    "recall_label_powerset = recall_score(pd.DataFrame(y_val), y_pred_label_powerset, average='micro')\n",
    "f1_label_powerset = f1_score(pd.DataFrame(y_val), y_pred_label_powerset, average='micro')\n",
    "\n",
    "# Calculate evaluation metrics for Classifier Chains\n",
    "accuracy_classifier_chain = accuracy_score(pd.DataFrame(y_val), y_pred_classifier_chain)\n",
    "precision_classifier_chain = precision_score(pd.DataFrame(y_val), y_pred_classifier_chain, average='micro')\n",
    "recall_classifier_chain = recall_score(pd.DataFrame(y_val), y_pred_classifier_chain, average='micro')\n",
    "f1_classifier_chain = f1_score(pd.DataFrame(y_val), y_pred_classifier_chain, average='micro')\n",
    "\n",
    "\n",
    "# # Display the evaluation metrics\n",
    "# print(\"Binary Relevance:\")\n",
    "# print(\"Accuracy:\", accuracy_binary_relevance)\n",
    "# print(\"Precision:\", precision_binary_relevance)\n",
    "# print(\"Recall:\", recall_binary_relevance)\n",
    "# print(\"F1 Score:\", f1_binary_relevance)\n",
    "# print(\"Hamming Loss:\", hamming_loss_binary_relevance)\n",
    "# print(\"G-mean:\", gmean_binary_relevance)\n",
    "# #print(\"One-error:\", one_error_binary_relevance)\n",
    "# print(\"Coverage:\", coverage_binary_relevance)\n",
    "\n",
    "print(\"Binary Relevance:\")\n",
    "print(\"Accuracy:\", round(accuracy_binary_relevance, 4))\n",
    "print(\"Precision:\", round(precision_binary_relevance, 4))\n",
    "print(\"Recall:\", round(recall_binary_relevance, 4))\n",
    "print(\"F1 Score:\", round(f1_binary_relevance, 4))\n",
    "print(\"Hamming Loss:\", round(hamming_loss_binary_relevance, 4))\n",
    "print(\"G-mean:\", round(gmean_binary_relevance, 4))\n",
    "#print(\"One-error:\", round(one_error_binary_relevance, 4))\n",
    "print(\"Coverage:\", round(coverage_binary_relevance, 4))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nLabel Powerset:\")\n",
    "# print(\"Accuracy:\", accuracy_label_powerset)\n",
    "# print(\"Precision:\", precision_label_powerset)\n",
    "# print(\"Recall:\", recall_label_powerset)\n",
    "# print(\"F1 Score:\", f1_label_powerset)\n",
    "# print(\"Hamming Loss:\", hamming_loss_label_powerset)\n",
    "# print(\"G-mean:\", gmean_label_powerset)\n",
    "# #print(\"One-error:\", one_error_label_powerset)\n",
    "# print(\"Coverage:\", coverage_label_powerset)\n",
    "\n",
    "print(\"\\nLabel Powerset:\")\n",
    "print(\"Accuracy:\", round(accuracy_label_powerset, 4))\n",
    "print(\"Precision:\", round(precision_label_powerset, 4))\n",
    "print(\"Recall:\", round(recall_label_powerset, 4))\n",
    "print(\"F1 Score:\", round(f1_label_powerset, 4))\n",
    "print(\"Hamming Loss:\", round(hamming_loss_label_powerset, 4))\n",
    "print(\"G-mean:\", round(gmean_label_powerset, 4))\n",
    "#print(\"One-error:\", one_error_label_powerset)\n",
    "print(\"Coverage:\", round(coverage_label_powerset, 4))\n",
    "\n",
    "# print(\"\\nClassifier Chains:\")\n",
    "# print(\"Accuracy:\", accuracy_classifier_chain)\n",
    "# print(\"Precision:\", precision_classifier_chain)\n",
    "# print(\"Recall:\", recall_classifier_chain)\n",
    "# print(\"F1 Score:\", f1_classifier_chain)\n",
    "# print(\"Hamming Loss:\", hamming_loss_classifier_chain)\n",
    "# print(\"G-mean:\", gmean_classifier_chain)\n",
    "# #print(\"One-error:\", one_error_classifier_chain)\n",
    "# print(\"Coverage:\", coverage_classifier_chain)\n",
    "\n",
    "print(\"\\nClassifier Chains:\")\n",
    "print(\"Accuracy:\", round(accuracy_classifier_chain, 4))\n",
    "print(\"Precision:\", round(precision_classifier_chain, 4))\n",
    "print(\"Recall:\", round(recall_classifier_chain, 4))\n",
    "print(\"F1 Score:\", round(f1_classifier_chain, 4))\n",
    "print(\"Hamming Loss:\", round(hamming_loss_classifier_chain, 4))\n",
    "print(\"G-mean:\", round(gmean_classifier_chain, 4))\n",
    "#print(\"One-error:\", one_error_classifier_chain)\n",
    "print(\"Coverage:\", round(coverage_classifier_chain, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981e0fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb179da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536366d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34618a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d8a287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import random\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ba29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Google's pre-trained Word2Vec model\n",
    "word2vec_path = '/Users/deepika/Documents/Masters/Courses/Practicum/GoogleNews-vectors-negative300.bin'  # Provide the path to the downloaded Word2Vec binary file\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88bb1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_data = pd.read_csv('data/train.csv')\n",
    "train_data['TEXT'] = train_data['TITLE'] + ' ' + train_data['ABSTRACT']\n",
    "\n",
    "X = train_data['TEXT']\n",
    "y = train_data[['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']].values\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to convert text to word embeddings\n",
    "def text_to_wv(text, model):\n",
    "    vectors = []\n",
    "    for word in text.split():\n",
    "        if word in model:\n",
    "            vectors.append(model[word])\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Convert text data to word embeddings\n",
    "X_train_wv = np.array([text_to_wv(text, word2vec_model) for text in X_train])\n",
    "X_val_wv = np.array([text_to_wv(text, word2vec_model) for text in X_val])\n",
    "\n",
    "# Helper function to identify minority labels\n",
    "def get_tail_labels(y):\n",
    "    tail_labels = [i for i in range(y.shape[1]) if np.sum(y[:, i]) < (y.shape[0] / 2)]\n",
    "    return tail_labels\n",
    "\n",
    "# class distribution before applying dynamic MLSMOTE\n",
    "print(\"Class distribution before applying dynamic MLSMOTE:\")\n",
    "for i, label in enumerate(['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']):\n",
    "    print(f\"{label}: {np.sum(y_train[:, i])}\")\n",
    "\n",
    "# Dynamic MLSMOTE function\n",
    "def dynamic_MLSMOTE(X, y, target_balance=4500):\n",
    "    n_neighbors = min(5, len(X) - 1)\n",
    "    neigh = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "    neigh.fit(X)\n",
    "    tail_labels = get_tail_labels(y)\n",
    "    synthetic_samples = []\n",
    "    synthetic_labels = []\n",
    "\n",
    "    for i in tail_labels:\n",
    "        current_count = np.sum(y[:, i])\n",
    "        n_samples = max(target_balance - current_count, 0)  # Calculate the number of samples to generate\n",
    "        target_indices = np.where(y[:, i] == 1)[0]\n",
    "        \n",
    "        if len(target_indices) >= n_neighbors:\n",
    "            nn = neigh.kneighbors(X[target_indices], return_distance=False)\n",
    "            for _ in range(n_samples):\n",
    "                sample_index = random.choice(range(len(target_indices)))\n",
    "                nn_indices = nn[sample_index, 1:]\n",
    "                chosen_nn = random.choice(nn_indices)\n",
    "                step = np.random.rand()\n",
    "                synthetic_sample = X[target_indices[sample_index]] + step * (X[chosen_nn] - X[target_indices[sample_index]])\n",
    "                synthetic_samples.append(synthetic_sample)\n",
    "                synthetic_label = y[target_indices[sample_index]].copy()\n",
    "                synthetic_labels.append(synthetic_label)\n",
    "    \n",
    "    if len(synthetic_samples) > 0:\n",
    "        X_synthetic = np.vstack(synthetic_samples)\n",
    "        y_synthetic = np.vstack(synthetic_labels)\n",
    "        X_balanced = np.vstack((X, X_synthetic))\n",
    "        y_balanced = np.vstack((y, y_synthetic))\n",
    "        return X_balanced, y_balanced\n",
    "    else:\n",
    "        return X, y\n",
    "\n",
    "# Convert y_train to numpy array for processing\n",
    "y_train_np = y_train\n",
    "\n",
    "# Adjust this target balance\n",
    "target_balance = 4000 \n",
    "X_balanced_wv, y_balanced = dynamic_MLSMOTE(X_train_wv, y_train_np, target_balance=target_balance)\n",
    "#X_balanced_wv, y_balanced = dynamic_MLSMOTE(X_train, y_train_np, target_balance=target_balance)\n",
    "\n",
    "# class distribution after applying dynamic MLSMOTE\n",
    "print(\"\\n\")\n",
    "print(\"Class distribution after applying dynamic MLSMOTE:\")\n",
    "for i, label in enumerate(['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']):\n",
    "    print(f\"{label}: {np.sum(y_balanced[:, i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c78b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_balanced_wv))\n",
    "print(type(y_balanced))\n",
    "print(type(X_val_wv))\n",
    "print(type(y_val))\n",
    "\n",
    "\n",
    "np.save('data/X_balanced_wv_Word2Vec.npy', X_balanced_wv)\n",
    "np.save('data/y_balanced_Word2Vec.npy', y_balanced)\n",
    "np.save('data/X_val_wv_Word2Vec.npy', X_val_wv)\n",
    "np.save('data/y_val_Word2Vec.npy', y_val)\n",
    "\n",
    "X_balanced_wv = np.load('data/X_balanced_wv_Word2Vec.npy')\n",
    "y_balanced = np.load('data/y_balanced_Word2Vec.npy')\n",
    "X_val_wv = np.load('data/X_val_wv_Word2Vec.npy')\n",
    "y_val = np.load('data/y_val_Word2Vec.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea71bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, f1_score\n",
    "from skmultilearn.problem_transform import BinaryRelevance, LabelPowerset, ClassifierChain\n",
    "import numpy as np\n",
    "\n",
    "# Define the additional evaluation metrics functions\n",
    "def geometric_mean_score(y_true, y_pred):\n",
    "    gmean = np.sqrt(accuracy_score(y_true, y_pred, normalize=True) *\n",
    "                     accuracy_score(y_true, y_pred, normalize=True))\n",
    "    return gmean\n",
    "\n",
    "def balanced_accuracy_score(y_true, y_pred):\n",
    "    return balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "def one_error(y_true, y_pred):\n",
    "    incorrect = np.sum(np.logical_and((y_true == 0), (y_pred == 1)))\n",
    "    one_err = incorrect / y_true.shape[0]\n",
    "    return one_err\n",
    "\n",
    "\n",
    "def coverage_error(y_true, y_pred):\n",
    "    cov_err = np.mean(np.sum(y_pred, axis=1))\n",
    "    return cov_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e9ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base classifier\n",
    "base_classifier = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Binary Relevance\n",
    "binary_relevance_classifier = BinaryRelevance(classifier=base_classifier)\n",
    "binary_relevance_classifier.fit(X_balanced_wv, y_balanced)\n",
    "y_pred_binary_relevance = binary_relevance_classifier.predict(X_val_wv)\n",
    "\n",
    "# Label Powerset\n",
    "label_powerset_classifier = LabelPowerset(classifier=base_classifier)\n",
    "label_powerset_classifier.fit(X_balanced_wv, y_balanced)\n",
    "y_pred_label_powerset = label_powerset_classifier.predict(X_val_wv)\n",
    "\n",
    "# Classifier Chains\n",
    "classifier_chain_classifier = ClassifierChain(classifier=base_classifier)\n",
    "classifier_chain_classifier.fit(X_balanced_wv, y_balanced)\n",
    "y_pred_classifier_chain = classifier_chain_classifier.predict(X_val_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843af7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics for Binary Relevance\n",
    "accuracy_binary_relevance = accuracy_score(y_val, y_pred_binary_relevance)\n",
    "hamming_loss_binary_relevance = hamming_loss(y_val, y_pred_binary_relevance)\n",
    "f1_binary_relevance = f1_score(y_val, y_pred_binary_relevance, average='micro')\n",
    "gmean_binary_relevance = geometric_mean_score(y_val, y_pred_binary_relevance)\n",
    "one_error_binary_relevance = one_error(y_val, y_pred_binary_relevance)\n",
    "coverage_binary_relevance = coverage_error(y_val, y_pred_binary_relevance)\n",
    "\n",
    "# Calculate evaluation metrics for Label Powerset\n",
    "accuracy_label_powerset = accuracy_score(y_val, y_pred_label_powerset)\n",
    "hamming_loss_label_powerset = hamming_loss(y_val, y_pred_label_powerset)\n",
    "f1_label_powerset = f1_score(y_val, y_pred_label_powerset, average='micro')\n",
    "gmean_label_powerset = geometric_mean_score(y_val, y_pred_label_powerset)\n",
    "one_error_label_powerset = one_error(y_val, y_pred_label_powerset)\n",
    "coverage_label_powerset = coverage_error(y_val, y_pred_label_powerset)\n",
    "\n",
    "# Calculate evaluation metrics for Classifier Chains\n",
    "accuracy_classifier_chain = accuracy_score(y_val, y_pred_classifier_chain)\n",
    "hamming_loss_classifier_chain = hamming_loss(y_val, y_pred_classifier_chain)\n",
    "f1_classifier_chain = f1_score(y_val, y_pred_classifier_chain, average='micro')\n",
    "gmean_classifier_chain = geometric_mean_score(y_val, y_pred_classifier_chain)\n",
    "one_error_classifier_chain = one_error(y_val, y_pred_classifier_chain)\n",
    "coverage_classifier_chain = coverage_error(y_val, y_pred_classifier_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6045174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking_loss(y_true, y_pred): \n",
    "    loss = 0\n",
    "    for i in range(len(y_true)):\n",
    "        diff = np.sum(np.abs(y_true[i] - y_pred[i]))\n",
    "        loss += diff / (len(y_true[i]) * (len(y_true[i]) - 1))\n",
    "    return loss / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f273a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate evaluation metrics for Binary Relevance\n",
    "accuracy_binary_relevance = accuracy_score(pd.DataFrame(y_val), y_pred_binary_relevance)\n",
    "precision_binary_relevance = precision_score(pd.DataFrame(y_val), y_pred_binary_relevance, average='micro')\n",
    "recall_binary_relevance = recall_score(pd.DataFrame(y_val), y_pred_binary_relevance, average='micro')\n",
    "f1_binary_relevance = f1_score(pd.DataFrame(y_val), y_pred_binary_relevance, average='micro')\n",
    "\n",
    "# Calculate evaluation metrics for Label Powerset\n",
    "accuracy_label_powerset = accuracy_score(pd.DataFrame(y_val), y_pred_label_powerset)\n",
    "precision_label_powerset = precision_score(pd.DataFrame(y_val), y_pred_label_powerset, average='micro')\n",
    "recall_label_powerset = recall_score(pd.DataFrame(y_val), y_pred_label_powerset, average='micro')\n",
    "f1_label_powerset = f1_score(pd.DataFrame(y_val), y_pred_label_powerset, average='micro')\n",
    "\n",
    "# Calculate evaluation metrics for Classifier Chains\n",
    "accuracy_classifier_chain = accuracy_score(pd.DataFrame(y_val), y_pred_classifier_chain)\n",
    "precision_classifier_chain = precision_score(pd.DataFrame(y_val), y_pred_classifier_chain, average='micro')\n",
    "recall_classifier_chain = recall_score(pd.DataFrame(y_val), y_pred_classifier_chain, average='micro')\n",
    "f1_classifier_chain = f1_score(pd.DataFrame(y_val), y_pred_classifier_chain, average='micro')\n",
    "\n",
    "\n",
    "# # Display the evaluation metrics\n",
    "# print(\"Binary Relevance:\")\n",
    "# print(\"Accuracy:\", accuracy_binary_relevance)\n",
    "# print(\"Precision:\", precision_binary_relevance)\n",
    "# print(\"Recall:\", recall_binary_relevance)\n",
    "# print(\"F1 Score:\", f1_binary_relevance)\n",
    "# print(\"Hamming Loss:\", hamming_loss_binary_relevance)\n",
    "# print(\"G-mean:\", gmean_binary_relevance)\n",
    "# #print(\"One-error:\", one_error_binary_relevance)\n",
    "# print(\"Coverage:\", coverage_binary_relevance)\n",
    "\n",
    "print(\"Binary Relevance:\")\n",
    "print(\"Accuracy:\", round(accuracy_binary_relevance, 4))\n",
    "print(\"Precision:\", round(precision_binary_relevance, 4))\n",
    "print(\"Recall:\", round(recall_binary_relevance, 4))\n",
    "print(\"F1 Score:\", round(f1_binary_relevance, 4))\n",
    "print(\"Hamming Loss:\", round(hamming_loss_binary_relevance, 4))\n",
    "print(\"G-mean:\", round(gmean_binary_relevance, 4))\n",
    "#print(\"One-error:\", round(one_error_binary_relevance, 4))\n",
    "print(\"Coverage:\", round(coverage_binary_relevance, 4))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nLabel Powerset:\")\n",
    "# print(\"Accuracy:\", accuracy_label_powerset)\n",
    "# print(\"Precision:\", precision_label_powerset)\n",
    "# print(\"Recall:\", recall_label_powerset)\n",
    "# print(\"F1 Score:\", f1_label_powerset)\n",
    "# print(\"Hamming Loss:\", hamming_loss_label_powerset)\n",
    "# print(\"G-mean:\", gmean_label_powerset)\n",
    "# #print(\"One-error:\", one_error_label_powerset)\n",
    "# print(\"Coverage:\", coverage_label_powerset)\n",
    "\n",
    "print(\"\\nLabel Powerset:\")\n",
    "print(\"Accuracy:\", round(accuracy_label_powerset, 4))\n",
    "print(\"Precision:\", round(precision_label_powerset, 4))\n",
    "print(\"Recall:\", round(recall_label_powerset, 4))\n",
    "print(\"F1 Score:\", round(f1_label_powerset, 4))\n",
    "print(\"Hamming Loss:\", round(hamming_loss_label_powerset, 4))\n",
    "print(\"G-mean:\", round(gmean_label_powerset, 4))\n",
    "#print(\"One-error:\", one_error_label_powerset)\n",
    "print(\"Coverage:\", round(coverage_label_powerset, 4))\n",
    "\n",
    "# print(\"\\nClassifier Chains:\")\n",
    "# print(\"Accuracy:\", accuracy_classifier_chain)\n",
    "# print(\"Precision:\", precision_classifier_chain)\n",
    "# print(\"Recall:\", recall_classifier_chain)\n",
    "# print(\"F1 Score:\", f1_classifier_chain)\n",
    "# print(\"Hamming Loss:\", hamming_loss_classifier_chain)\n",
    "# print(\"G-mean:\", gmean_classifier_chain)\n",
    "# #print(\"One-error:\", one_error_classifier_chain)\n",
    "# print(\"Coverage:\", coverage_classifier_chain)\n",
    "\n",
    "print(\"\\nClassifier Chains:\")\n",
    "print(\"Accuracy:\", round(accuracy_classifier_chain, 4))\n",
    "print(\"Precision:\", round(precision_classifier_chain, 4))\n",
    "print(\"Recall:\", round(recall_classifier_chain, 4))\n",
    "print(\"F1 Score:\", round(f1_classifier_chain, 4))\n",
    "print(\"Hamming Loss:\", round(hamming_loss_classifier_chain, 4))\n",
    "print(\"G-mean:\", round(gmean_classifier_chain, 4))\n",
    "#print(\"One-error:\", one_error_classifier_chain)\n",
    "print(\"Coverage:\", round(coverage_classifier_chain, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42093227",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1d825de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, hamming_loss,coverage_error, confusion_matrix\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.metrics import coverage_error\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8b5c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before applying dynamic MLSMOTE:\n",
      "Computer Science: 6902\n",
      "Physics: 4787\n",
      "Mathematics: 4468\n",
      "Statistics: 4137\n",
      "Quantitative Biology: 465\n",
      "Quantitative Finance: 204\n",
      "\n",
      "\n",
      "Class distribution after applying dynamic MLSMOTE:\n",
      "Computer Science: 7562\n",
      "Physics: 4793\n",
      "Mathematics: 4585\n",
      "Statistics: 5727\n",
      "Quantitative Biology: 4554\n",
      "Quantitative Finance: 4526\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import random\n",
    "\n",
    "# Load the dataset\n",
    "#train_data = pd.read_csv('train.csv')\n",
    "train_data = pd.read_csv('data/train.csv')\n",
    "train_data['TEXT'] = train_data['TITLE'] + ' ' + train_data['ABSTRACT']\n",
    "\n",
    "X = train_data['TEXT']\n",
    "y = train_data[['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']].values\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train).toarray()\n",
    "X_val_tfidf = vectorizer.transform(X_val).toarray()\n",
    "\n",
    "# Helper function to identify minority labels\n",
    "def get_tail_labels(y):\n",
    "    tail_labels = [i for i in range(y.shape[1]) if np.sum(y[:, i]) < (y.shape[0] / 2)]\n",
    "    return tail_labels\n",
    "\n",
    "# class distribution before applying dynamic MLSMOTE\n",
    "print(\"Class distribution before applying dynamic MLSMOTE:\")\n",
    "for i, label in enumerate(['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']):\n",
    "    print(f\"{label}: {np.sum(y_train[:, i])}\")\n",
    "\n",
    "# Dynamic MLSMOTE function\n",
    "def dynamic_MLSMOTE(X, y, target_balance=4500):\n",
    "    n_neighbors = min(5, len(X) - 1)\n",
    "    neigh = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "    neigh.fit(X)\n",
    "    tail_labels = get_tail_labels(y)\n",
    "    synthetic_samples = []\n",
    "    synthetic_labels = []\n",
    "\n",
    "    for i in tail_labels:\n",
    "        current_count = np.sum(y[:, i])\n",
    "        n_samples = max(target_balance - current_count, 0)  # Calculate the number of samples to generate\n",
    "        target_indices = np.where(y[:, i] == 1)[0]\n",
    "        \n",
    "        if len(target_indices) >= n_neighbors:\n",
    "            nn = neigh.kneighbors(X[target_indices], return_distance=False)\n",
    "            for _ in range(n_samples):\n",
    "                sample_index = random.choice(range(len(target_indices)))\n",
    "                nn_indices = nn[sample_index, 1:]\n",
    "                chosen_nn = random.choice(nn_indices)\n",
    "                step = np.random.rand()\n",
    "                synthetic_sample = X[target_indices[sample_index]] + step * (X[chosen_nn] - X[target_indices[sample_index]])\n",
    "                synthetic_samples.append(synthetic_sample)\n",
    "                synthetic_label = y[target_indices[sample_index]].copy()\n",
    "                synthetic_labels.append(synthetic_label)\n",
    "    \n",
    "    if len(synthetic_samples) > 0:\n",
    "        X_synthetic = np.vstack(synthetic_samples)\n",
    "        y_synthetic = np.vstack(synthetic_labels)\n",
    "        X_balanced = np.vstack((X, X_synthetic))\n",
    "        y_balanced = np.vstack((y, y_synthetic))\n",
    "        return X_balanced, y_balanced\n",
    "    else:\n",
    "        return X, y\n",
    "\n",
    "# Convert y_train to numpy array for processing\n",
    "y_train_np = y_train\n",
    "\n",
    "# Adjust this target balance\n",
    "target_balance = 4500  \n",
    "X_balanced_tfidf, y_balanced = dynamic_MLSMOTE(X_train_tfidf, y_train_np, target_balance=target_balance)\n",
    "\n",
    "# class distribution after applying dynamic MLSMOTE\n",
    "print(\"\\n\")\n",
    "print(\"Class distribution after applying dynamic MLSMOTE:\")\n",
    "for i, label in enumerate(['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']):\n",
    "    print(f\"{label}: {np.sum(y_balanced[:, i])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3808eb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_balanced_tfidf))\n",
    "print(type(y_balanced))\n",
    "print(type(X_val_tfidf))\n",
    "print(type(y_val))\n",
    "\n",
    "\n",
    "np.save('data/X_balanced_tfidf.npy', X_balanced_tfidf)\n",
    "np.save('data/y_balanced_tfidf.npy', y_balanced)\n",
    "np.save('data/X_val_tfidf.npy', X_val_tfidf)\n",
    "np.save('data/y_val_tfidf.npy', y_val)\n",
    "\n",
    "X_balanced_tfidf = np.load('data/X_balanced_tfidf.npy')\n",
    "y_balanced = np.load('data/y_balanced_tfidf.npy')\n",
    "X_val_tfidf = np.load('data/X_val_tfidf.npy')\n",
    "y_val = np.load('data/y_val_tfidf.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fbea5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, f1_score\n",
    "from skmultilearn.problem_transform import BinaryRelevance, LabelPowerset, ClassifierChain\n",
    "import numpy as np\n",
    "\n",
    "# Define the additional evaluation metrics functions\n",
    "def geometric_mean_score(y_true, y_pred):\n",
    "    gmean = np.sqrt(accuracy_score(y_true, y_pred, normalize=True) *\n",
    "                     accuracy_score(y_true, y_pred, normalize=True))\n",
    "    return gmean\n",
    "\n",
    "def balanced_accuracy_score(y_true, y_pred):\n",
    "    return balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "def one_error(y_true, y_pred):\n",
    "    incorrect = np.sum(np.logical_and((y_true == 0), (y_pred == 1)))\n",
    "    one_err = incorrect / y_true.shape[0]\n",
    "    return one_err\n",
    "\n",
    "\n",
    "def coverage_error(y_true, y_pred):\n",
    "    cov_err = np.mean(np.sum(y_pred, axis=1))\n",
    "    return cov_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9b7e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base classifier\n",
    "base_classifier = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Binary Relevance\n",
    "binary_relevance_classifier = BinaryRelevance(classifier=base_classifier)\n",
    "binary_relevance_classifier.fit(X_balanced_tfidf, y_balanced)\n",
    "y_pred_binary_relevance = binary_relevance_classifier.predict(X_val_tfidf)\n",
    "\n",
    "# Label Powerset\n",
    "label_powerset_classifier = LabelPowerset(classifier=base_classifier)\n",
    "label_powerset_classifier.fit(X_balanced_tfidf, y_balanced)\n",
    "y_pred_label_powerset = label_powerset_classifier.predict(X_val_tfidf)\n",
    "\n",
    "# Classifier Chains\n",
    "classifier_chain_classifier = ClassifierChain(classifier=base_classifier)\n",
    "classifier_chain_classifier.fit(X_balanced_tfidf, y_balanced)\n",
    "y_pred_classifier_chain = classifier_chain_classifier.predict(X_val_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098984b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics for Binary Relevance\n",
    "accuracy_binary_relevance = accuracy_score(y_val, y_pred_binary_relevance)\n",
    "hamming_loss_binary_relevance = hamming_loss(y_val, y_pred_binary_relevance)\n",
    "f1_binary_relevance = f1_score(y_val, y_pred_binary_relevance, average='micro')\n",
    "gmean_binary_relevance = geometric_mean_score(y_val, y_pred_binary_relevance)\n",
    "one_error_binary_relevance = one_error(y_val, y_pred_binary_relevance)\n",
    "coverage_binary_relevance = coverage_error(y_val, y_pred_binary_relevance)\n",
    "\n",
    "# Calculate evaluation metrics for Label Powerset\n",
    "accuracy_label_powerset = accuracy_score(y_val, y_pred_label_powerset)\n",
    "hamming_loss_label_powerset = hamming_loss(y_val, y_pred_label_powerset)\n",
    "f1_label_powerset = f1_score(y_val, y_pred_label_powerset, average='micro')\n",
    "gmean_label_powerset = geometric_mean_score(y_val, y_pred_label_powerset)\n",
    "one_error_label_powerset = one_error(y_val, y_pred_label_powerset)\n",
    "coverage_label_powerset = coverage_error(y_val, y_pred_label_powerset)\n",
    "\n",
    "# Calculate evaluation metrics for Classifier Chains\n",
    "accuracy_classifier_chain = accuracy_score(y_val, y_pred_classifier_chain)\n",
    "hamming_loss_classifier_chain = hamming_loss(y_val, y_pred_classifier_chain)\n",
    "f1_classifier_chain = f1_score(y_val, y_pred_classifier_chain, average='micro')\n",
    "gmean_classifier_chain = geometric_mean_score(y_val, y_pred_classifier_chain)\n",
    "one_error_classifier_chain = one_error(y_val, y_pred_classifier_chain)\n",
    "coverage_classifier_chain = coverage_error(y_val, y_pred_classifier_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b4a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking_loss(y_true, y_pred): \n",
    "    loss = 0\n",
    "    for i in range(len(y_true)):\n",
    "        diff = np.sum(np.abs(y_true[i] - y_pred[i]))\n",
    "        loss += diff / (len(y_true[i]) * (len(y_true[i]) - 1))\n",
    "    return loss / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13190c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate evaluation metrics for Binary Relevance\n",
    "accuracy_binary_relevance = accuracy_score(pd.DataFrame(y_val), y_pred_binary_relevance)\n",
    "precision_binary_relevance = precision_score(pd.DataFrame(y_val), y_pred_binary_relevance, average='micro')\n",
    "recall_binary_relevance = recall_score(pd.DataFrame(y_val), y_pred_binary_relevance, average='micro')\n",
    "f1_binary_relevance = f1_score(pd.DataFrame(y_val), y_pred_binary_relevance, average='micro')\n",
    "\n",
    "# Calculate evaluation metrics for Label Powerset\n",
    "accuracy_label_powerset = accuracy_score(pd.DataFrame(y_val), y_pred_label_powerset)\n",
    "precision_label_powerset = precision_score(pd.DataFrame(y_val), y_pred_label_powerset, average='micro')\n",
    "recall_label_powerset = recall_score(pd.DataFrame(y_val), y_pred_label_powerset, average='micro')\n",
    "f1_label_powerset = f1_score(pd.DataFrame(y_val), y_pred_label_powerset, average='micro')\n",
    "\n",
    "# Calculate evaluation metrics for Classifier Chains\n",
    "accuracy_classifier_chain = accuracy_score(pd.DataFrame(y_val), y_pred_classifier_chain)\n",
    "precision_classifier_chain = precision_score(pd.DataFrame(y_val), y_pred_classifier_chain, average='micro')\n",
    "recall_classifier_chain = recall_score(pd.DataFrame(y_val), y_pred_classifier_chain, average='micro')\n",
    "f1_classifier_chain = f1_score(pd.DataFrame(y_val), y_pred_classifier_chain, average='micro')\n",
    "\n",
    "\n",
    "# # Display the evaluation metrics\n",
    "# print(\"Binary Relevance:\")\n",
    "# print(\"Accuracy:\", accuracy_binary_relevance)\n",
    "# print(\"Precision:\", precision_binary_relevance)\n",
    "# print(\"Recall:\", recall_binary_relevance)\n",
    "# print(\"F1 Score:\", f1_binary_relevance)\n",
    "# print(\"Hamming Loss:\", hamming_loss_binary_relevance)\n",
    "# print(\"G-mean:\", gmean_binary_relevance)\n",
    "# #print(\"One-error:\", one_error_binary_relevance)\n",
    "# print(\"Coverage:\", coverage_binary_relevance)\n",
    "\n",
    "print(\"Binary Relevance:\")\n",
    "print(\"Accuracy:\", round(accuracy_binary_relevance, 4))\n",
    "print(\"Precision:\", round(precision_binary_relevance, 4))\n",
    "print(\"Recall:\", round(recall_binary_relevance, 4))\n",
    "print(\"F1 Score:\", round(f1_binary_relevance, 4))\n",
    "print(\"Hamming Loss:\", round(hamming_loss_binary_relevance, 4))\n",
    "print(\"G-mean:\", round(gmean_binary_relevance, 4))\n",
    "#print(\"One-error:\", round(one_error_binary_relevance, 4))\n",
    "print(\"Coverage:\", round(coverage_binary_relevance, 4))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\nLabel Powerset:\")\n",
    "# print(\"Accuracy:\", accuracy_label_powerset)\n",
    "# print(\"Precision:\", precision_label_powerset)\n",
    "# print(\"Recall:\", recall_label_powerset)\n",
    "# print(\"F1 Score:\", f1_label_powerset)\n",
    "# print(\"Hamming Loss:\", hamming_loss_label_powerset)\n",
    "# print(\"G-mean:\", gmean_label_powerset)\n",
    "# #print(\"One-error:\", one_error_label_powerset)\n",
    "# print(\"Coverage:\", coverage_label_powerset)\n",
    "\n",
    "print(\"\\nLabel Powerset:\")\n",
    "print(\"Accuracy:\", round(accuracy_label_powerset, 4))\n",
    "print(\"Precision:\", round(precision_label_powerset, 4))\n",
    "print(\"Recall:\", round(recall_label_powerset, 4))\n",
    "print(\"F1 Score:\", round(f1_label_powerset, 4))\n",
    "print(\"Hamming Loss:\", round(hamming_loss_label_powerset, 4))\n",
    "print(\"G-mean:\", round(gmean_label_powerset, 4))\n",
    "#print(\"One-error:\", one_error_label_powerset)\n",
    "print(\"Coverage:\", round(coverage_label_powerset, 4))\n",
    "\n",
    "# print(\"\\nClassifier Chains:\")\n",
    "# print(\"Accuracy:\", accuracy_classifier_chain)\n",
    "# print(\"Precision:\", precision_classifier_chain)\n",
    "# print(\"Recall:\", recall_classifier_chain)\n",
    "# print(\"F1 Score:\", f1_classifier_chain)\n",
    "# print(\"Hamming Loss:\", hamming_loss_classifier_chain)\n",
    "# print(\"G-mean:\", gmean_classifier_chain)\n",
    "# #print(\"One-error:\", one_error_classifier_chain)\n",
    "# print(\"Coverage:\", coverage_classifier_chain)\n",
    "\n",
    "print(\"\\nClassifier Chains:\")\n",
    "print(\"Accuracy:\", round(accuracy_classifier_chain, 4))\n",
    "print(\"Precision:\", round(precision_classifier_chain, 4))\n",
    "print(\"Recall:\", round(recall_classifier_chain, 4))\n",
    "print(\"F1 Score:\", round(f1_classifier_chain, 4))\n",
    "print(\"Hamming Loss:\", round(hamming_loss_classifier_chain, 4))\n",
    "print(\"G-mean:\", round(gmean_classifier_chain, 4))\n",
    "#print(\"One-error:\", one_error_classifier_chain)\n",
    "print(\"Coverage:\", round(coverage_classifier_chain, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3b1c30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
